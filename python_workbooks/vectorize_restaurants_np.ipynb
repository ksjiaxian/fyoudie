{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, 3)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    bert_out = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "    pooled_output = bert_out['pooler_output']\n",
    "    # print(pooled_output)\n",
    "    output = self.drop(pooled_output)\n",
    "    prob = F.softmax(self.out(output))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# model = torch.load('../models/bert_sentiment_model.pt', map_location=torch.device('cpu'))\n",
    "model = SentimentClassifier()\n",
    "model.load_state_dict(torch.load('../models/bert_sentiment_model.pt', map_location=torch.device('cpu')))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_sentiment(s):\n",
    "    text = s\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "    text,\n",
    "    max_length=MAX_LEN,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "\n",
    "    output = model(input_ids, attention_mask)\n",
    "    return int(torch.argmax(output) - 1)\n",
    "\n",
    "\n",
    "# in this function, we need the scalar, the n (number of rows and columns), and m, the row to apply the scalar to\n",
    "def generate_matrix(scalar, n, r):\n",
    "    mat = np.identity(n)\n",
    "    mat[:,r] *= scalar\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../pickles/masks.pkl\",\"rb\") as f:\n",
    "    mask_dict = pickle.load(f)\n",
    "# need to process this to be able to use it\n",
    "mask_vectors = list(mask_dict.values())\n",
    "mask_vectors = [i.todense() for i in mask_vectors]\n",
    "mask_matrix = np.stack(mask_vectors)\n",
    "\n",
    "with open(\"../pickles/restaurant_tfidf_dict.pkl\",\"rb\") as f:\n",
    "    rid_sid_dict = pickle.load(f)\n",
    "\n",
    "with open(\"../pickles/restaurant_sentence_list_dict.pkl\",\"rb\") as f:\n",
    "    rid_tfidf_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % complete\n",
      "10.0 % complete\n",
      "20.0 % complete\n",
      "30.0 % complete\n",
      "40.0 % complete\n",
      "50.0 % complete\n",
      "60.0 % complete\n",
      "70.0 % complete\n",
      "80.0 % complete\n",
      "90.0 % complete\n"
     ]
    }
   ],
   "source": [
    "# iterate through the rids\n",
    "rids = list(rid_sid_dict.keys())\n",
    "num_rids = len(rids)\n",
    "\n",
    "rid_feature_dict = {}\n",
    "for i in range(num_rids):\n",
    "    # progress tracker\n",
    "    if (i % 10 == 0) :\n",
    "        print((i / num_rids) * 100, '% complete')\n",
    "        \n",
    "    rid = rids[i]\n",
    "    # get the sentences in order\n",
    "    sentences = list(rid_sid_dict[rid].values)\n",
    "    sentiments = np.array([get_sentence_sentiment(s) for s in sentences])\n",
    "    \n",
    "#     print('sentiments calculated: ', len(sentiments))\n",
    "     \n",
    "    tfidf_matrix = rid_tfidf_dict[rid]\n",
    "    tfidf_matrix_w = tfidf_matrix.copy()\n",
    "    tfidf_matrix_w = (tfidf_matrix_w.T.multiply(sentiments)).T # multiply the sentiments through for weighted matrix\n",
    "    tfidf_matrix_w = np.array(tfidf_matrix_w.todense())\n",
    "    tfidf_matrix = np.array(tfidf_matrix.todense())\n",
    "        \n",
    "    # sum over all columns of the matrix, for both weighted and not weighted\n",
    "    freq_vec = np.sum(tfidf_matrix, axis=0)\n",
    "    freq_vec_w = np.sum(tfidf_matrix_w, axis=0)\n",
    "    \n",
    "    # normalize the vectors\n",
    "    norm = np.linalg.norm(freq_vec)\n",
    "    norm_w = np.linalg.norm(freq_vec_w)\n",
    "    freq_vec = freq_vec / norm\n",
    "    freq_vec_w = freq_vec_w / norm_w\n",
    "    \n",
    "    # apply the mask for params and save both weighted and non weighted vectors for this rid\n",
    "    rid_feature_dict[rid] = (np.matmul(mask_matrix, freq_vec.T).T, np.matmul(mask_matrix, freq_vec_w.T).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the dictionary\n",
    "with open('../pickles/rid_feature_dict.pkl', 'wb') as handle:\n",
    "    pickle.dump(rid_feature_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('X9sz3xeaLujW9PhsSLCQyg', 0.14871434114858917),\n",
       " ('VPqWLp9kMiZEbctCebIZUA', 0.1345436262560355),\n",
       " ('kvXWN5kB7CEzmMCf557Xug', 0.1256071647116429),\n",
       " ('JgjMHbZ4A407ZOmvEEoa6g', 0.11327886708919),\n",
       " ('GDs0ymtRPWWHlUMBfNT5yg', 0.10766604287171508),\n",
       " ('SCwzUgW_RUdahXNObknkTg', 0.10476257632362315),\n",
       " ('p2BkIrOuIsxGqtV0lwOZUw', 0.10181158419779535),\n",
       " ('eUbq0uNxRlXQ6sy7phM7yA', 0.0980889325752394),\n",
       " ('AvXqLbcGCxdIEF_qZTY0Kw', 0.09803051194041147),\n",
       " ('8zehGz9jnxPqXtOc7KaJxA', 0.09634001588805935),\n",
       " ('C--wIpxJ4j1y01G_raHdbA', 0.09223970154339176),\n",
       " ('DiRIdYhGyuTNZurKyuWf7A', 0.08739411298674277),\n",
       " ('zmZ3HkVCeZPBefJJxzdJ7A', 0.08159229670766677),\n",
       " ('0WqROvvlHjvpeHIP0fg9EQ', 0.08073619180284945),\n",
       " ('CoZmZKv2lCYd-UoAsAUobA', 0.07794962362088068),\n",
       " ('hpz2qRnei2IJROLJUJVvPQ', 0.07574199166562945),\n",
       " ('paHbKmPjwirnIP7esoTPCQ', 0.07479472345968627),\n",
       " ('mOnesB4IF9j6-ZmHoOHOig', 0.07209447350170148),\n",
       " ('DMC9ZMkDHNQmlhdPDmz-Cw', 0.07206927920029216),\n",
       " ('xdpH27x6qGSG21LLa6TaXQ', 0.07130060436626756),\n",
       " ('bZiIIUcpgxh8mpKMDhdqbA', 0.06781515012174022),\n",
       " ('J8Ha6yIvGoU-E31jnCq7Ew', 0.06732615071023372),\n",
       " ('jhFi0oxdyiyCYK7i-CW59A', 0.06540475057377133),\n",
       " ('0BGoel6on7yGvojzOqOEAQ', 0.0635434669690915),\n",
       " ('Un6u2cECyV4nZb_HGZ-uTA', 0.06333321347186721),\n",
       " ('2ILe62hVJfiZEOcKxFYKDA', 0.06109222944171699),\n",
       " ('72PQGMhrEcIuWH-S44TprA', 0.060603209723528846),\n",
       " ('H_RM2u1WWGU1HkKZrYq2Ow', 0.05961734214685202),\n",
       " ('EmJOkTKIwgm7QJbls7hN3w', 0.0571781232074542),\n",
       " ('gvpoFnaCq2J7TBXV7yFIkQ', 0.0519287504058846),\n",
       " ('zg9p8ZBFmEUCtfcBHNvzvg', 0.051261546559795784),\n",
       " ('V16Mua3SnXLt95N4EEolTw', 0.05085848087071811),\n",
       " ('PrWSjn4a8o4dHoqKs53GBA', 0.05068344142701811),\n",
       " ('tiKV9b0b2kpaGm5a-nd1ig', 0.04942938441158816),\n",
       " ('-L69Ix0-xX4BlHA61fGvrQ', 0.04898764839958384),\n",
       " ('IdXHHEUH4ebcxdRxCo3JNw', 0.04869082933858156),\n",
       " ('DbXHNl890xSXNiyRczLWAg', 0.04806763860572764),\n",
       " ('mw_qxZJraNu7Q6u0GkcMew', 0.04708313564506622),\n",
       " ('75HV-KqCtn_oHeiLiGlO_w', 0.0433351882654372),\n",
       " ('OlB0841vj4V7Wje5tTtLWA', 0.042493698797507924),\n",
       " ('jMz_y_-cWMfiZF7Q5snE6Q', 0.04131479086766775),\n",
       " ('LAhRM37ofCq5f4nCM20aCA', 0.04115047615771997),\n",
       " ('WQFn1A7-UAA4JT5YWiop_w', 0.040743175003267365),\n",
       " ('YZs1gNSh_sN8JmN_nrpxeA', 0.04073078999806795),\n",
       " ('M4jkJHewQXZvDV34Tuon8g', 0.04046858599084713),\n",
       " ('TA1KUSCu8GkWP9w0rmElxw', 0.04038724412965916),\n",
       " ('_QUh5vFHSuw8R_uiFZ7XKQ', 0.03962572015618156),\n",
       " ('9P-lp3AWDXGayDqJz9VPwQ', 0.036210085930575106),\n",
       " ('hrvQyPfec2YqNk9ZknlYig', 0.03393016754042337),\n",
       " ('-_GnwXmzC3DXsHR9nyaC2g', 0.03145220318330777),\n",
       " ('gHvqhb6TjIupKZyot0DIgA', 0.03128906673426915),\n",
       " ('ZW7aI5FO_3q_vSzI4_zx-Q', 0.030655712133601968),\n",
       " ('MH15A8iP8JSQRTXvYfqi8w', 0.029874419637671137),\n",
       " ('Y6xGZZIQEtwjCkVHL-_GRg', 0.02815599543738013),\n",
       " ('ch1ercqwoNLpQLxpTb90KQ', 0.02790550589914198),\n",
       " ('LGZimmjxbfGwBHgPZCx5cQ', 0.027726837529517642),\n",
       " ('wVlxPWwFL-MrzCXim-UfWg', 0.026894000649484537),\n",
       " ('dWBKYjQ3q2v2dOjsfLLxDg', 0.026478166627602873),\n",
       " ('G61xj19TPnbzB7oQuoWTgQ', 0.02580979422243468),\n",
       " ('LUCmOKTK5Uh7eU84yJq3mQ', 0.025302331327172856),\n",
       " ('g3pXGPRKYd_WQxLT2IFPxQ', 0.02391550687905052),\n",
       " ('F7xXy4c9pCSQYVG_QxeBSw', 0.02311883751346834),\n",
       " ('6Hm2FmfLcU_M91TrZI5htA', 0.021785980667228075),\n",
       " ('ZKT4K5aAhE3KC6xPG0phYw', 0.021740675656516766),\n",
       " ('oaaeZ4BvMR9pHXYb5USEEQ', 0.021293202743780895),\n",
       " ('4cgE4DUDSH3CjqsRwX_LOA', 0.020266829205005138),\n",
       " ('KXCXaF5qimmtKKqnPc_LQA', 0.01963776483031577),\n",
       " ('VWuI68p6Ao_ENKaCGRazLA', 0.019382119717853988),\n",
       " ('xPo2OA5_wQgUEzAr-1o1Lg', 0.019034550068255846),\n",
       " ('r1vkjKMksmHhE9SjvSAdaw', 0.018819014394918576),\n",
       " ('jGennaZUr2MsJyRhijNBfA', 0.01864954630850508),\n",
       " ('1BvysshfkDS2eJ0k8XiDjQ', 0.017529788894287735),\n",
       " ('PYDzfxSLUCCwQysbOkFSVA', 0.01534315011617346),\n",
       " ('JO4tK5oH6ncQ5m_MqkwxVQ', 0.012037468311735815),\n",
       " ('luOZQ9YBrWwP8mYrS4rNoA', 0.008228647785662783),\n",
       " ('ARDNlZlbB1c3F-Qvk3lJXQ', 0.006697351043543867),\n",
       " ('I7atR1yTJMOgSoZLXLtCWA', 0.0016026868685166742),\n",
       " ('Qydynzc1ApxsSkVNEgPs9A', -7.993834418240642e-06),\n",
       " ('EEHhKSxUvJkoPSzeGKkpVg', -0.0038443181992772885),\n",
       " ('WtDOs3a6k_oPJmwiDh4qBQ', -0.006086968682298487),\n",
       " ('0AzLzHfOJgL7ROwhdww2ew', -0.00812689972167683),\n",
       " ('FQsIpsHVYuvE8jH5jrE5Lw', -0.008489976341598017),\n",
       " ('vV5ciKb5sDKVmF8mAoz1iA', -0.01279565404488427),\n",
       " ('EuiU19EjhybpGLXqBzntgw', -0.013391509427268687),\n",
       " ('EXOsmAB1s71WePlQk0WZrA', -0.014246365133884591),\n",
       " ('ll5v-nUVgMsTMUC-MBGRFQ', -0.016738921783500796),\n",
       " ('HUEZLPIM64cDZnfHE1H3AA', -0.020079995222731722),\n",
       " ('gGvNgShksetPoimyKV8I9Q', -0.023645011981314835),\n",
       " ('Je42YeoIWbMUQphd-UsKLA', -0.02493107103955115),\n",
       " ('WDGeeyeK7bG0cvq_ZglAdA', -0.025537996637219725),\n",
       " ('xGXzsc-hzam-VArK6eTvtw', -0.026403276750096703),\n",
       " ('buF9druCkbuXLX526sGELQ', -0.029059820595011814),\n",
       " ('JCgLWYjoPJlznzFEtnzqKg', -0.0292082320620108),\n",
       " ('05pmc_4J0TxoZrft1QxmJg', -0.032779912732647555),\n",
       " ('avhO2gx4ensf6Su6ld9d5g', -0.03840703873731619),\n",
       " ('SNn6O_vN8EtR-UFYtzYZ-Q', -0.04451811358757342),\n",
       " ('qbpJFE-XlspCCk3PWhZ0AA', -0.049718055317665724),\n",
       " ('4mmKcjlUWF3NTjtgrmBMFQ', -0.058004096040804705),\n",
       " ('UAtX7xmIfdd1W2Pebf6NWg', -0.06291283918370444),\n",
       " ('lq0rcFfgZJfzo9UD8bjDuA', -0.07857239989866313)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rid_feature_dict)\n",
    "\n",
    "high_so_far = 0\n",
    "high_rid = None\n",
    "rid_scores = []\n",
    "for key, value in rid_feature_dict.items():\n",
    "    rid_scores.append((key, sum(value[1]).item()))\n",
    "    if sum(value[1]).item() > high_so_far:\n",
    "        high_so_far = sum(value[1]).item()\n",
    "        high_rid = key\n",
    "\n",
    "rid_scores = sorted(rid_scores, key=lambda x: x[1], reverse = True)\n",
    "rid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[0.06492115],\n",
       "         [0.00284262]]),\n",
       " matrix([[0.04342017],\n",
       "         [0.00556748]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rid_feature_dict['zmZ3HkVCeZPBefJJxzdJ7A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
