{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, 3)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    bert_out = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "    pooled_output = bert_out['pooler_output']\n",
    "    # print(pooled_output)\n",
    "    output = self.drop(pooled_output)\n",
    "    prob = F.softmax(self.out(output))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('../models/bert_sentiment_model.pt', map_location=torch.device('cpu'))\n",
    "model = SentimentClassifier()\n",
    "model.load_state_dict(torch.load('../models/bert_sentiment_model.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_sentiment(s):\n",
    "    text = s\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "    text,\n",
    "    max_length=MAX_LEN,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "\n",
    "    output = model(input_ids, attention_mask)\n",
    "    return int(torch.argmax(output) - 1)\n",
    "\n",
    "\n",
    "# in this function, we need the scalar, the n (number of rows and columns), and m, the row to apply the scalar to\n",
    "def generate_matrix(scalar, n, r):\n",
    "    mat = np.identity(n)\n",
    "    mat[:,r] *= scalar\n",
    "    return mat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagine that we have 2 dictionaries\n",
    "# one dictionary is rid to list of sentences\n",
    "# one dictionary is rid to tf idf matrix\n",
    "\n",
    "# iterate through the rids\n",
    "rids = rid_sid_dict.keys()\n",
    "\n",
    "rid_feature_dict = {}\n",
    "for rid in rids:\n",
    "    # get the sentences in order\n",
    "    sentences = rid_sid_dict[rid]\n",
    "    sentiments = map(get_sentence_sentiment, sentences)\n",
    "    \n",
    "    # this is the number of rows in the tf idf matrix\n",
    "    n = len(sentiments)\n",
    "    \n",
    "    # create the matrices for sentiments (to be able to multiply through)\n",
    "    sentiment_matrices = []\n",
    "    for i in range(0, n):\n",
    "        sentiment_matrices.append(generate_matrix[sentiments[i], n, i])\n",
    "        \n",
    "    tfidf_matrix = rid_tfidf_dict[rid]\n",
    "    tfidf_matrix_w = tfidf_matrix.copy()\n",
    "    \n",
    "    # for each sentiment matrix, multiply it by the tfidf weighted\n",
    "    for m in sentiment_matrices:\n",
    "        tfidf_matrix_w = np.matmul(m, tfidf_matrix_w)\n",
    "        \n",
    "    # sum over all columns of the  m\n",
    "    # sum over all columns of the matrix, for both weighted and not weighted\n",
    "    freq_vec = np.sum(tfidf_matrix, axis=0)\n",
    "    freq_vec_w = np.sum(tfidf_matrix_w, axis=0)\n",
    "    \n",
    "    # apply the mask for params and save both weighted and non weighted vectors for this rid\n",
    "    rid_feature_dict[rid] = (np.matmul(param_mask, freq_vec.T).T, np.matmul(param_mask, freq_vec_w.T).T)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
